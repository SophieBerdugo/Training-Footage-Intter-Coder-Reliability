---
title: "VM Training Footage Inter-Coder Reliability"
author: "Sophie Berdugo"
date: "06/05/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Efficiency Variables

### Video UID: 8cd3b20b-f527-4092-907c-0311c8f3cfa6
### Video Data and Time: 2010-01-03 | 08:05:33

**Rater 1: Sophie Berdugo**
**Rater 2: Victoire Martignac**

Load libraries for analysis
```{r message=FALSE, warning=FALSE}
library(tidyverse) #Load tidyverse to use the tidyverse package
library(irr) #Load the irr package to retrieve packages relating to inter-rater reliability.
```

Import data frames from csv files.
```{r message=FALSE, warning=FALSE}
sb_training <- read_csv("~/Documents/Oxford/DPhil/Project Components/Inter-Coder Reliability/Data/Victoire Martignac/Training Data/SB_Clean.csv") #read csv file for SB clean training data
sb_training <- slice(sb_training, 1:58) #create new data frame of subset of first 58 rows
vm_training <- read_csv("~/Documents/Oxford/DPhil/Project Components/Inter-Coder Reliability/Data/Victoire Martignac/Training Data/VM_Clean.csv") #read csv file for VM clean training data
sb_training #print
vm_training
```

# Variables
This analysis involves establishing the levels of inter-rater reliability for various variables relating to adult nut-cracking efficiency and model investment. These variables are:

* Bout outcome
* Strikes per nut
* Displacement rate
* Tool switch rate
* Bout duration

The inter-rater reliability analysis utilised depends on the level of measurement of each variable under investigation. The levels of measurement and corresponding test of inter rater reliability for each variable can be found in Table 1, below:

*Table 1*

| Variable                | Level of Measurement | Approach                           |
| :---------------------- | :------------------- | :--------------------------------- |
| Bout outcome            | Nominal              | Cohen's kappa                      |
| Strikes per nut         | Ratio                | Intraclass correlation coefficient |
| Displacement rate       | Ratio                | Intraclass correlation coefficient |
| Tool switch rate        | Ratio                | Intraclass correlation coefficient |
| Bout duration           | Ratio                | Intraclass correlation coefficient |

### Bout outcome

The three potential bout outcomes are:

1. *Successful* - The nut is cracked open using the hammer and anvil composite and the full kernel is retrieved and eaten by an individual (not necessarily by focal subject, if kernel is scrounged). 
2. *Smash* - The nut is cracked open using hammer and anvil, but the kernel is smashed into multiple pieces and each piece is eaten separately.
3. *Failed* - A different nut is placed on the anvil following displacement of the original nut; or, the nut is not retrieved or not eaten by any individual. 

This data is nominal so an unweighted Cohen's kappa will be used to establish the concordance between two independent observers. 

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_outcome <- select(sb_training, "Bout Outcome") #select the column for bout outcome from sb data frame
vm_outcome <- select(vm_training, "Bout Outcome") #select the column for bout outcome from vm data frame
bout_outcome <- bind_cols(sb_outcome, vm_outcome) #combine these two columns
bout_outcome <- bout_outcome %>%
  rename(
    SB = "Bout Outcome...1",
    VM = "Bout Outcome...2"
  ) #rename the two columns and override original data frame of bout_outcome to include renamed columns
bout_outcome #print
```

Calculate Cohen's kappa for two coders.

```{r}
kappa2(bout_outcome[,c(1,2)], "unweighted")
```
**The agreement between the two raters was moderate, κ = 0.502, and greater than would be expected by chance,** ***Z*** **= 3.93,** ***p*** **< .05**

### Bout outcome following consultation between coders and clarification of coding scheme

```{r}
sb_vm_outcome_2 <- read_csv("~/Documents/Oxford/DPhil/Project Components/Inter-Coder Reliability/Data/Victoire Martignac/Training Data/SB_VM_outcome_retrained.csv")
sb_vm_outcome_2
```
Calculate Cohen's kappa for two coders.

```{r}
kappa2(sb_vm_outcome_2[,c(1,2)], "unweighted")
```
**The agreement between the two raters was almost perfect, κ = 0.937, and greater than would be expected by chance,** ***Z*** **= 7.55,** ***p*** **< .05**

### Number of strikes

The variable *number of strikes* records the number of strikes performed by the focal individual on a nut in the whole nut-cracking bout. This is an exact number that is manually inputted by the coder. 

This data is ratio, and so intraclass correlations (ICC) will be used to establish the reliability between two independent coders. In accordance with Koo & Li's (2016) guidelines on ICC model selection, the method used here will be a two-way, mixed-effects, single rater absolute agreement ICC.

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_strikes <- select(sb_training, "Number of Strikes") #select the column for number of strikes from sb data frame
vm_strikes <- select(vm_training, "Number of Stikes") #select the column for number of strikes from vm data frame
strikes <- bind_cols(sb_strikes, vm_strikes) #combine these two columns
strikes <- strikes %>%
  rename(
    SB = "Number of Strikes",
    VM = "Number of Stikes"
  ) #rename the two columns and override original data frame of strikes to include renamed columns
strikes #print
```

Calculate ICC for two coders
```{r}
icc(strikes, #data frame
    model = "twoway", #two-way model selected as the selected raters are the only raters of interest (Koo & Li, 2016)
    type = "agreement", #absolute agreement between the raters to be calculated
    unit = "single", #measurement from a single rater as the basis of the actual measurement
    r0 = 0, #specification of the null hypothesis
    conf.level = 0.95) #confidence level
```
**ICC estimates and their 95% confidence intervals were calculated using R (v. 4.0.3) based on a single rater (k = 2), absolute-agreement, two-way mixed effects model. The ICC score indicated excellent reliability.** 

### Displacement rate

The variable *displacement rate* indicates the number of times a nut is hit off the anvil before the kernel is retrieved. This data is ratio, and so intraclass correlations (ICC) will be used to establish the reliability between two independent coders. In accordance with Koo & Li's (2016) guidelines on ICC model selection, the method used here will be a two-way, mixed-effects, single rater absolute agreement ICC.

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_displacement <- select(sb_training, "Displacement rate") #select the column for displacement rate from sb data frame
vm_displacement <- select(vm_training, "Displacement rate") #select the column for displacement rate from vm data frame
displacement <- bind_cols(sb_displacement, vm_displacement) #combine these two columns
displacement <- displacement %>%
  rename(
    SB = "Displacement rate...1",
    VM = "Displacement rate...2"
  ) #rename the two columns and override original data frame of displacement to include renamed columns
displacement #print
```

Calculate ICC for two coders
```{r}
icc(displacement, #data frame
    model = "twoway", #two-way model selected as the selected raters are the only raters of interest (Koo & Li, 2016)
    type = "agreement", #absolute agreement between the raters to be calculated
    unit = "single", #measurement from a single rater as the basis of the actual measurement
    r0 = 0, #specification of the null hypothesis
    conf.level = 0.95) #confidence level
```
**ICC estimates and their 95% confidence intervals were calculated using R (v. 4.0.3) based on a single rater (k = 2), absolute-agreement, two-way mixed effects model. The ICC score indicated moderate reliability.** 

### Displacement rate following consultation between coders and clarification of coding scheme

Import data frame from csv file
```{r message=FALSE, warning=FALSE}
sb_vm_displacement_2 <- read_csv("~/Documents/Oxford/DPhil/Project Components/Inter-Coder Reliability/Data/Victoire Martignac/Training Data/SB_VM_displacements_retrained.csv")
sb_vm_displacement_2
```

Compute ICC for two coders
```{r}
icc(sb_vm_displacement_2, #data frame
    model = "twoway", #two-way model selected as the selected raters are the only raters of interest (Koo & Li, 2016)
    type = "consistency", #consistency between the raters to be calculated
    unit = "single", #measurement from a single rater as the basis of the actual measurement
    r0 = 0, #specification of the null hypothesis
    conf.level = 0.95) #confidence level
```
**ICC estimates and their 95% confidence intervals were calculated using R (v. 4.0.3) based on a single rater (k = 2), consistency, two-way mixed effects model. The ICC score indicated excellent reliability.** 

### Tool switch rate

The variable *tool switch rate* indicates the number of times a nut is hit off the anvil before the kernel is retrieved. This data is ratio, and so intraclass correlations (ICC) will be used to establish the reliability between two independent coders. In accordance with Koo & Li's (2016) guidelines on ICC model selection, the method used here will be a two-way, mixed-effects, single rater absolute agreement ICC.

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_tool_switch <- select(sb_training, "Tool switch rate") #select the column for tool switch rate from sb data frame
vm_tool_switch <- select(vm_training, "Tool switch rate") #select the column for tool switch rate from vm data frame
tool_switch <- bind_cols(sb_tool_switch, vm_tool_switch) #combine these two columns
tool_switch <- tool_switch %>%
  rename(
    SB = "Tool switch rate...1",
    VM = "Tool switch rate...2"
  ) #rename the two columns and override original data frame of tool switch to include renamed columns
tool_switch #print
```

Calculate ICC for two coders
```{r}
icc(tool_switch, #data frame
    model = "twoway", #two-way model selected as the selected raters are the only raters of interest (Koo & Li, 2016)
    type = "agreement", #absolute agreement between the raters to be calculated
    unit = "single", #measurement from a single rater as the basis of the actual measurement
    r0 = 0, #specification of the null hypothesis
    conf.level = 0.95) #confidence level
```
**ICC estimates and their 95% confidence intervals were calculated using R (v. 4.0.3) based on a single rater (k = 2), absolute-agreement, two-way mixed effects model. The ICC score indicated perfect reliability.** 

### Bout duration

The variable **bout duration** indicates the length in seconds that the individual was continuously striking a nut with a hammer and anvil composite before the nut was either retrieved or was not retreived. This data is ratio, and so intraclass correlations (ICC) will be used to establish the reliability between two independent coders. In accordance with Koo & Li's (2016) guidelines on ICC model selection, the method used here will be a two-way, mixed-effects, single rater, consistency ICC.

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_duration <- select(sb_training, "Bout Duration") #select the column for bout duration from sb data frame
vm_duration <- select(vm_training, "Bout Duration") #select the column for bout duration from vm data frame
duration <- bind_cols(sb_duration, vm_duration) #combine these two columns
duration <- duration %>%
  rename(
    SB = "Bout Duration...1",
    VM = "Bout Duration...2"
  ) #rename the two columns and override original data frame of duration to include renamed columns
duration #print
```

Calculate ICC for two coders
```{r}
icc(duration, #data frame
    model = "twoway", #two-way model selected as the selected raters are the only raters of interest (Koo & Li, 2016)
    type = "consistency", #consistency between the raters to be calculated
    unit = "single", #measurement from a single rater as the basis of the actual measurement
    r0 = 0, #specification of the null hypothesis
    conf.level = 0.95) #confidence level
```
**ICC estimates and their 95% confidence intervals were calculated using R (v. 4.0.3) based on a single rater (k = 2), consistency, two-way mixed effects model. The ICC score indicated excellent reliability.** 


## Model Investment Variables

### Video UID: 31e021b5-8ab6-450f-b5ef-73e2975b50d4
### Video Data: 2008-12-19

**Rater 1: Sophie Berdugo**
**Rater 2: Victoire Martignac**
