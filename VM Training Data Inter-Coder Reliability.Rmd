---
title: "VM Training Footage Inter-Coder Reliability"
author: "Sophie Berdugo"
date: "06/05/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Efficiency Variables

### Video UID: 8cd3b20b-f527-4092-907c-0311c8f3cfa6
### Video Data and Time: 2010-01-03 | 08:05:33

**Rater 1: Sophie Berdugo**
**Rater 2: Victoire Martignac**

Load libraries for analysis
```{r message=FALSE, warning=FALSE}
library(tidyverse) #Load tidyverse to use the tidyverse package
library(irr) #Load the irr package to retrieve packages relating to inter-rater reliability.
```

Import data frames from csv files.
```{r message=FALSE, warning=FALSE}
sb_training <- read_csv("~/Documents/Oxford/DPhil/Project Components/Inter-Coder Reliability/Data/Victoire Martignac/Training Data/SB_Clean.csv") #read csv file for SB clean training data
sb_training <- slice(sb_training, 1:58) #create new data frame of subset of first 58 rows
vm_training <- read_csv("~/Documents/Oxford/DPhil/Project Components/Inter-Coder Reliability/Data/Victoire Martignac/Training Data/VM_Clean.csv") #read csv file for VM clean training data
sb_training #print
vm_training
```

# Variables
This analysis involves establishing the levels of inter-rater reliability for various variables relating to adult nut-cracking efficiency and model investment. These variables are:

* Bout outcome
* Strikes per nut
* Displacement rate
* Tool switch rate
* Bout duration

The inter-rater reliability analysis utilised depends on the level of measurement of each variable under investigation. The levels of measurement and corresponding test of inter rater reliability for each variable can be found in Table 1, below:

*Table 1*

| Variable                | Level of Measurement | Approach                           |
| :---------------------- | :------------------- | :--------------------------------- |
| Bout outcome            | Nominal              | Cohen's kappa                      |
| Strikes per nut         | Ratio                | Intraclass correlation coefficient |
| Displacement rate       | Ratio                | Intraclass correlation coefficient |
| Tool switch rate        | Ratio                | Intraclass correlation coefficient |
| Bout duration           | Ratio                | Intraclass correlation coefficient |

### Bout outcome

The three potential bout outcomes are:

1. *Successful* - The nut is cracked open using the hammer and anvil composite and the full kernel is retrieved and eaten by an individual (not necessarily by focal subject, if kernel is scrounged). 
2. *Smash* - The nut is cracked open using hammer and anvil, but the kernel is smashed into multiple pieces and each piece is eaten separately.
3. *Failed* - A different nut is placed on the anvil following displacement of the original nut; or, the nut is not retrieved or not eaten by any individual. 

This data is nominal so an unweighted Cohen's kappa will be used to establish the concordance between two independent observers. 

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_outcome <- select(sb_training, "Bout Outcome") #select the column for bout outcome from sb data frame
vm_outcome <- select(vm_training, "Bout Outcome") #select the column for bout outcome from vm data frame
bout_outcome <- bind_cols(sb_outcome, vm_outcome) #combine these two columns
bout_outcome <- bout_outcome %>%
  rename(
    SB = "Bout Outcome...1",
    VM = "Bout Outcome...2"
  ) #rename the two columns and override original data frame of bout_outcome to include renamed columns
bout_outcome #print
```

Calculate Cohen's kappa for two coders.

```{r}
kappa2(bout_outcome[,c(1,2)], "unweighted")
```
**The agreement between the two raters was moderate, κ = 0.502, and greater than would be expected by chance,** ***Z*** **= 3.93,** ***p*** **< .05**

### Bout outcome following consultation between coders and clarification of coding scheme

```{r}
sb_vm_outcome_2 <- read_csv("~/Documents/Oxford/DPhil/Project Components/Inter-Coder Reliability/Data/Victoire Martignac/Training Data/SB_VM_outcome_retrained.csv")
sb_vm_outcome_2
```
Calculate Cohen's kappa for two coders.

```{r}
kappa2(sb_vm_outcome_2[,c(1,2)], "unweighted")
```
**The agreement between the two raters was almost perfect, κ = 0.937, and greater than would be expected by chance,** ***Z*** **= 7.55,** ***p*** **< .05**

### Number of strikes

The variable *number of strikes* records the number of strikes performed by the focal individual on a nut in the whole nut-cracking bout. This is an exact number that is manually inputted by the coder. 

This data is ratio, and so intraclass correlations (ICC) will be used to establish the reliability between two independent coders. In accordance with Koo & Li's (2016) guidelines on ICC model selection, the method used here will be a two-way, mixed-effects, single rater absolute agreement ICC.

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_strikes <- select(sb_training, "Number of Strikes") #select the column for number of strikes from sb data frame
vm_strikes <- select(vm_training, "Number of Stikes") #select the column for number of strikes from vm data frame
strikes <- bind_cols(sb_strikes, vm_strikes) #combine these two columns
strikes <- strikes %>%
  rename(
    SB = "Number of Strikes",
    VM = "Number of Stikes"
  ) #rename the two columns and override original data frame of strikes to include renamed columns
strikes #print
```

Calculate ICC for two coders
```{r}
icc(strikes, #data frame
    model = "twoway", #two-way model selected as the selected raters are the only raters of interest (Koo & Li, 2016)
    type = "agreement", #absolute agreement between the raters to be calculated
    unit = "single", #measurement from a single rater as the basis of the actual measurement
    r0 = 0, #specification of the null hypothesis
    conf.level = 0.95) #confidence level
```
**ICC estimates and their 95% confidence intervals were calculated using R (v. 4.0.3) based on a single rater (k = 2), absolute-agreement, two-way mixed effects model. The ICC score indicated excellent reliability.** 

### Displacement rate

The variable *displacement rate* indicates the number of times a nut is hit off the anvil before the kernel is retrieved. This data is ratio, and so intraclass correlations (ICC) will be used to establish the reliability between two independent coders. In accordance with Koo & Li's (2016) guidelines on ICC model selection, the method used here will be a two-way, mixed-effects, single rater absolute agreement ICC.

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_displacement <- select(sb_training, "Displacement rate") #select the column for displacement rate from sb data frame
vm_displacement <- select(vm_training, "Displacement rate") #select the column for displacement rate from vm data frame
displacement <- bind_cols(sb_displacement, vm_displacement) #combine these two columns
displacement <- displacement %>%
  rename(
    SB = "Displacement rate...1",
    VM = "Displacement rate...2"
  ) #rename the two columns and override original data frame of displacement to include renamed columns
displacement #print
```

Calculate ICC for two coders
```{r}
icc(displacement, #data frame
    model = "twoway", #two-way model selected as the selected raters are the only raters of interest (Koo & Li, 2016)
    type = "agreement", #absolute agreement between the raters to be calculated
    unit = "single", #measurement from a single rater as the basis of the actual measurement
    r0 = 0, #specification of the null hypothesis
    conf.level = 0.95) #confidence level
```
**ICC estimates and their 95% confidence intervals were calculated using R (v. 4.0.3) based on a single rater (k = 2), absolute-agreement, two-way mixed effects model. The ICC score indicated moderate reliability.** 

### Displacement rate following consultation between coders and clarification of coding scheme

Import data frame from csv file
```{r message=FALSE, warning=FALSE}
sb_vm_displacement_2 <- read_csv("~/Documents/Oxford/DPhil/Project Components/Inter-Coder Reliability/Data/Victoire Martignac/Training Data/SB_VM_displacements_retrained.csv")
sb_vm_displacement_2
```

Compute ICC for two coders
```{r}
icc(sb_vm_displacement_2, #data frame
    model = "twoway", #two-way model selected as the selected raters are the only raters of interest (Koo & Li, 2016)
    type = "consistency", #consistency between the raters to be calculated
    unit = "single", #measurement from a single rater as the basis of the actual measurement
    r0 = 0, #specification of the null hypothesis
    conf.level = 0.95) #confidence level
```
**ICC estimates and their 95% confidence intervals were calculated using R (v. 4.0.3) based on a single rater (k = 2), consistency, two-way mixed effects model. The ICC score indicated excellent reliability.** 

### Tool switch rate

The variable *tool switch rate* indicates the number of times a nut is hit off the anvil before the kernel is retrieved. This data is ratio, and so intraclass correlations (ICC) will be used to establish the reliability between two independent coders. In accordance with Koo & Li's (2016) guidelines on ICC model selection, the method used here will be a two-way, mixed-effects, single rater absolute agreement ICC.

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_tool_switch <- select(sb_training, "Tool switch rate") #select the column for tool switch rate from sb data frame
vm_tool_switch <- select(vm_training, "Tool switch rate") #select the column for tool switch rate from vm data frame
tool_switch <- bind_cols(sb_tool_switch, vm_tool_switch) #combine these two columns
tool_switch <- tool_switch %>%
  rename(
    SB = "Tool switch rate...1",
    VM = "Tool switch rate...2"
  ) #rename the two columns and override original data frame of tool switch to include renamed columns
tool_switch #print
```

Calculate ICC for two coders
```{r}
icc(tool_switch, #data frame
    model = "twoway", #two-way model selected as the selected raters are the only raters of interest (Koo & Li, 2016)
    type = "agreement", #absolute agreement between the raters to be calculated
    unit = "single", #measurement from a single rater as the basis of the actual measurement
    r0 = 0, #specification of the null hypothesis
    conf.level = 0.95) #confidence level
```
**ICC estimates and their 95% confidence intervals were calculated using R (v. 4.0.3) based on a single rater (k = 2), absolute-agreement, two-way mixed effects model. The ICC score indicated perfect reliability.** 

### Bout duration

The variable **bout duration** indicates the length in seconds that the individual was continuously striking a nut with a hammer and anvil composite before the nut was either retrieved or was not retreived. This data is ratio, and so intraclass correlations (ICC) will be used to establish the reliability between two independent coders. In accordance with Koo & Li's (2016) guidelines on ICC model selection, the method used here will be a two-way, mixed-effects, single rater, consistency ICC.

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_duration <- select(sb_training, "Bout Duration") #select the column for bout duration from sb data frame
vm_duration <- select(vm_training, "Bout Duration") #select the column for bout duration from vm data frame
duration <- bind_cols(sb_duration, vm_duration) #combine these two columns
duration <- duration %>%
  rename(
    SB = "Bout Duration...1",
    VM = "Bout Duration...2"
  ) #rename the two columns and override original data frame of duration to include renamed columns
duration #print
```

Calculate ICC for two coders
```{r}
icc(duration, #data frame
    model = "twoway", #two-way model selected as the selected raters are the only raters of interest (Koo & Li, 2016)
    type = "consistency", #consistency between the raters to be calculated
    unit = "single", #measurement from a single rater as the basis of the actual measurement
    r0 = 0, #specification of the null hypothesis
    conf.level = 0.95) #confidence level
```
**ICC estimates and their 95% confidence intervals were calculated using R (v. 4.0.3) based on a single rater (k = 2), consistency, two-way mixed effects model. The ICC score indicated excellent reliability.** 

## Model Investment Variables

### Video UID: 31e021b5-8ab6-450f-b5ef-73e2975b50d4
### Video Data: 2008-12-19

**Rater 1: Sophie Berdugo**
**Rater 2: Victoire Martignac**

**Variables**
This analysis involves establishing the levels of inter-rater reliability for various variables relating to model investment. These variables are:

* Learner
* Maternal model
* Learner name
* Model tolerance
* Model intolerance

All of these variables are nominal level data so unweighted Cohen's κ will be used to establish the concordance between two independent observers. 

### Learner

The *learner* variable describes whether or not there was a conspecific who was present during the focal individual's nut-cracking bout. The three options for coding were:

1. *Watching* - There is a learner watching the focal subject during the nut-cracking bout, whereby the learner’s face is oriented toward the model’s hand or arm during the bout.
2. *Interacting* - There is a learner present, and they are interacting with the focal subject (for example, touching arm or body of model, climbing on model).
3. *Clinging* - There is a learner present, and they are clinging to the focal subject or sat on the focal subject’s lap and not watching the nut-cracking bout. 
4. *Clinging and watching* - There is a learner present, and they are clinging to the focal subject or sat on the focal subject’s lap and watching the nut-cracking bout. 
5. *Present* - There is a learner present, ≤ 1 metre (m) away from the focal subject during the bout, but they are not watching the bout or interacting with the model. 
6. *Solo* - There is no learner with the focal individual during their nut-cracking bout.

### Maternal model

The *maternal model* variable describes whether the model is the learner's mother or not. The three options for coding are:

1. *Yes*
2. *No*
3. *None* - recorded if no learner was present

### Learner name

The *learner name* variable records the name of the learner if one is present in the bout. The coding options for the variable are the names 'None' is recorded if no learner was present.

### Model tolerance

The variable *model tolerance* is a record of whether the focal subject permitted any of the following learner behaviours during their nut-cracking bout:

1. *None* - No learner attempted to scrounge the kernel, interact with the model or the model’s tools or be proximate to the model during the nut-cracking bout.
2. *Scrounge* -	The focal chimpanzee permitted the learner to take the extracted kernel to eat themselves.
3. *Tool interaction* -	The focal chimpanzee permitted the learner to touch either one or both stones in the tool composite during the nut-cracking bout.
4. *Model interaction* - The focal chimpanzee permitted the learner to interact with them (for example, touching arm or body of model, climbing on model) during their nut-cracking bout.
5. *Tool re-use* - The focal chimpanzee permitted the learner to re-use one or both of their tool composite tools after their nut-cracking bout. 
6. *Proximity* - The focal chimpanzee permitted the learner to be ≤ 1 metre (m) away from them during the bout. In this case, the learner was not interacting with the focal subject, but was merely close to the focal subject and watching the nut-cracking bout.

### Model intolerance

The variable *model intolerance* is a record of whether the focal subject expressed intolerance toward a learner. The possible behaviours are:

1. *None* -	The focal subject did not express intolerance toward a learner during the bout.
2. *Physical* -	The focal subject physically rejected a learner’s attempts to scrounge, interact with the model or the tools, or be proximate whilst they were nut-cracking (for example, shoving, hitting, or chasing the learner).
3. *Vocal* -	The focal subject vocally rejected a learner’s attempts to scrounge, interact with the model or the tools, or be proximate whilst they are nut-cracking (for example, grunting, barking or screaming). Barking sounds like ‘waa’, ‘waaoo’ or ‘aaoo’, and are shorter than screams (Crockford & Boesch, 2003, p. 117).
