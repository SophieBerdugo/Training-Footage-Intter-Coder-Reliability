---
title: "VM Training Footage Inter-Coder Reliability"
author: "Sophie Berdugo"
date: "06/05/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Video UID: 8cd3b20b-f527-4092-907c-0311c8f3cfa6
### Video Data and Time: 2010-01-03 | 08:05:33

**Rater 1: Sophie Berdugo**
**Rater 2: Victoire Martignac**

Load libraries for analysis
```{r message=FALSE, warning=FALSE}
library(tidyverse) #Load tidyverse to use the tidyverse package
library(irr) #Load the irr package to retrieve packages relating to inter-rater reliability.
```

Import data frames from csv files.
```{r message=FALSE, warning=FALSE}
sb_training <- read_csv("~/Documents/Oxford/DPhil/Project Components/Inter-Coder Reliability/Data/Victoire Martignac/Training Data/SB_Clean.csv") #read csv file for SB clean training data
sb_training <- slice(sb_training, 1:58) #create new data frame of subset of first 58 rows
vm_training <- read_csv("~/Documents/Oxford/DPhil/Project Components/Inter-Coder Reliability/Data/Victoire Martignac/Training Data/VM_Clean.csv") #read csv file for VM clean training data
sb_training #print
vm_training
```

# Variables
This analysis involves establishing the levels of inter-rater reliability for various variables relating to adult nut-cracking efficiency and model investment. These variables are:

* Bout outcome
* Strikes per nut
* Displacement rate
* Tool switch rate
* Bout duration

The inter-rater reliability analysis utilised depends on the level of measurement of each variable under investigation. The levels of measurement and corresponding test of inter rater reliability for each variable can be found in Table 1, below:

*Table 1*

| Variable                | Level of Measurement | Approach                           |
| :---------------------- | :------------------- | :--------------------------------- |
| Bout outcome            | Nominal              | Cohen's kappa                      |
| Strikes per nut         | Ratio                | Intraclass correlation coefficient |
| Displacement rate       | Ratio                | Intraclass correlation coefficient |
| Tool switch rate        | Ratio                | Intraclass correlation coefficient |
| Bout duration           | Ratio                | Intraclass correlation coefficient |

### Bout outcome

The three potential bout outcomes are:

1. *Successful* - The nut is cracked open using the hammer and anvil composite and the full kernel is retrieved and eaten by an individual (not necessarily by focal subject, if kernel is scrounged). 
2. *Smash* - The nut is cracked open using hammer and anvil, but the kernel is smashed into multiple pieces and each piece is eaten separately.
3. *Failed* - A different nut is placed on the anvil following displacement of the original nut; or, the nut is not retrieved or not eaten by any individual. 

This data is nominal so an unweighted Cohen's kappa will be used to establish the concordance between two independent observers. 

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_outcome <- select(sb_training, "Bout Outcome") #select the column for bout outcome from sb data frame
vm_outcome <- select(vm_training, "Bout Outcome") #select the column for bout outcome from vm data frame
bout_outcome <- bind_cols(sb_outcome, vm_outcome) #combine these two columns
bout_outcome <- bout_outcome %>%
  rename(
    SB = "Bout Outcome...1",
    VM = "Bout Outcome...2"
  ) #rename the two columns and override original data frame of bout_outcome to include renamed columns
bout_outcome #print
```

Calculate Cohen's kappa for two coders.

```{r}
kappa2(bout_outcome[,c(1,2)], "unweighted")
```
**The agreement between the two raters was moderate, Îº = 0.502, and greater than would be expected by chance,** ***Z*** **= 3.93,** ***p*** **< .05**

### Number of strikes

The variable *number of strikes* records the number of strikes performed by the focal individual on a nut in the whole nut-cracking bout. This is an exact number that is manually inputted by the coder. 

This data is ratio, and so intraclass correlations (ICC) will be used to establish the reliability between two independent coders. In accordance with Koo & Li's (2016) guidelines on ICC model selection, the method used here will be a two-way, mixed-effects, single rater absolute agreement ICC.

Create data frame of bout outcome ready for Cohen's kappa calculation
```{r}
sb_strikes <- select(sb_training, "Number of Strikes") #select the column for number of strikes from sb data frame
vm_strikes <- select(vm_training, "Number of Stikes") #select the column for number of strikes from vm data frame
strikes <- bind_cols(sb_strikes, vm_strikes) #combine these two columns
strikes <- strikes %>%
  rename(
    SB = "Number of Strikes",
    VM = "Number of Stikes"
  ) #rename the two columns and override original data frame of bout_outcome to include renamed columns
strikes #print
```

Calculate ICC for two coders
```{r}
icc(strikes, #data frame
    model = "twoway", #two-way model selected as the selected raters are the only raters of interest (Koo & Li, 2016)
    type = "agreement", #absolute agreement between the raters to be calculated
    unit = "single", #measurement from a single rater as the basis of the actual measurement
    r0 = 0, #specification of the null hypothesis
    conf.level = 0.95) #confidence level
```
**ICC estimates and their 95% confidence intervals were calculated using R (v. 4.0.3) based on a single rater (k = 2), absolute-agreement, two-way mixed effects model. The ICC score indicated excellent reliability.** 

